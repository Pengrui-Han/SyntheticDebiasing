{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyP1tk451B/z5n7IvSL7WXKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barryhpr/SyntheticDebiasing/blob/main/Adapter_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvhRh-JBN38m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1aeef0-041f-449d-a941-d072b8ea485d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2023.7.22)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.26.1\n",
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-3.2.1-py3-none-any.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (2023.7.22)\n",
            "Installing collected packages: adapter-transformers\n",
            "Successfully installed adapter-transformers-3.2.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.5\n",
            "Cloning into 'SyntheticDebiasing'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 69 (delta 29), reused 61 (delta 28), pack-reused 7\u001b[K\n",
            "Receiving objects: 100% (69/69), 6.37 MiB | 12.05 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n",
            "/content/SyntheticDebiasing\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.26.1\n",
        "!pip install -U adapter-transformers\n",
        "!pip install datasets\n",
        "!git clone https://github.com/barryhpr/SyntheticDebiasing.git\n",
        "%cd /content/SyntheticDebiasing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Score for GPT2"
      ],
      "metadata": {
        "id": "QgfNI1Y3slhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Score_evaluator import *\n",
        "from Score_evaluator1_1 import *\n",
        "from Evaluate_StereoSet import *\n",
        "from Evaluate_CrowSPairs import *\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "#Input file: \"test.json\" to evaluate all categories. For a specific category, use for example: \"test_gender.json\".\n",
        "srunner = StereoSetRunner(\n",
        "    intrasentence_model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    # model_name = \"gpt2\",\n",
        "    input_file=\"test.json\",\n",
        "    is_generative=True,\n",
        "    is_self_debias=False\n",
        "    )\n",
        "\n",
        "metric_data = srunner() # a nested dict\n",
        "\n",
        "import json\n",
        "with open('stereoset_result.json', 'w') as outfile:\n",
        "    json.dump(metric_data, outfile, indent=4)\n",
        "\n",
        "parse_file(\"test.json\", \"stereoset_result.json\")\n",
        "# parse_file1(\"test.json\",\"stereoset_result.json\", \"gender\")"
      ],
      "metadata": {
        "id": "4S0nc5XkOg9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train GPT2"
      ],
      "metadata": {
        "id": "Bn3IHT6rsfUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "from Evaluate_CrowSPairs import CrowSPairsRunner\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, filename, tokenizer):\n",
        "        with open(filename, 'r') as file:\n",
        "            self.data = json.load(file)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = f\"{item['prompt'].replace('{}', item['subject'])} {item['target_new']['str']}\"\n",
        "        encoding = self.tokenizer(text, return_tensors='pt')\n",
        "        labels = encoding['input_ids'].clone()\n",
        "        labels[:, :-1] = -100\n",
        "        return {'input_ids': encoding['input_ids'].squeeze(), 'attention_mask': encoding['attention_mask'].squeeze(), 'labels': labels.squeeze()}\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#Datasets are like \"Synthetic_Religion_3.json\", remember don't use S. So for exmaple, don't use \"Synthetic_Religion_3S\"\n",
        "train_dataset = MyDataset('Synthetic_Gender_2.json', tokenizer)\n",
        "test_dataset = MyDataset('test.json', tokenizer)\n",
        "\n",
        "if os.path.exists('./results'):\n",
        "    shutil.rmtree('./results')\n",
        "os.makedirs('./results')\n",
        "\n",
        "\n",
        "# Training session\n",
        "training_epoch = 5\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.add_adapter(\"debias\")\n",
        "model.train_adapter(\"debias\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=training_epoch,\n",
        "    per_device_train_batch_size=1,\n",
        "    logging_steps=10,\n",
        "    seed=42,\n",
        "    learning_rate=5e-6\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "eiHDkyHXN82I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate GPT2 Model on StereoSet & CrowSPairs"
      ],
      "metadata": {
        "id": "b40dtE7FsZwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation session\n",
        "from Score_evaluator import *\n",
        "from Score_evaluator1_1 import *\n",
        "from Evaluate_StereoSet import *\n",
        "from Evaluate_CrowSPairs import *\n",
        "evaluation_list = [1,2,3]\n",
        "for eval_epoch in evaluation_list:\n",
        "\n",
        "    print(f\"Evaluate epoch{eval_epoch}\")\n",
        "    #this should be 500 for normal training\n",
        "    checkpoint_path = f\"/content/SyntheticDebiasing/results/checkpoint-{eval_epoch*1000}\"\n",
        "    eval_model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
        "    eval_model.set_active_adapters(\"debias\")\n",
        "\n",
        "    ##########################################################\n",
        "\n",
        "# please import input_file eg. \"test-religion.json\" for evaluating religion bias\n",
        "\n",
        "    srunner = StereoSetRunner(\n",
        "        intrasentence_model = eval_model,\n",
        "        tokenizer = tokenizer,\n",
        "        input_file=\"test-race.json\",\n",
        "        is_generative=True,\n",
        "        is_self_debias=False\n",
        "        )\n",
        "\n",
        "    metric_data = srunner() # a nested dict\n",
        "\n",
        "    import json\n",
        "    with open('stereoset_result.json', 'w') as outfile:\n",
        "        json.dump(metric_data, outfile, indent=4)  # The `indent=4` makes the JSON output more readable\n",
        "\n",
        "    parse_file1(\"test.json\",\n",
        "\t\t'stereoset_result.json', \"race\")\n",
        "\n",
        "\n",
        "    #########################################################\n",
        "    runner = CrowSPairsRunner(\n",
        "        model=eval_model,\n",
        "        tokenizer=tokenizer,\n",
        "        input_file=\"crows_pairs_anonymized.csv\",\n",
        "        bias_type=\"race\",\n",
        "        is_generative=True\n",
        "    )\n",
        "\n",
        "    results = runner()\n",
        "    print(f\"Metric Score: {results}\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "TvI9fnKuOj4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate GPT2 Model on BiasTestGPT"
      ],
      "metadata": {
        "id": "dNhacbz5sS10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "## Applicable datasets names ###\n",
        "\n",
        "# Formatted_Profession_vs_Gender\n",
        "# Formatted_Math_Arts_vs_Male_Female\n",
        "# Formatted_Mexican_Female_European_Male_vs_MFemergent_EMintersectional\n",
        "# Formatted_YoungName_OldName_vs_Pleasant_Unpleasant\n",
        "\n",
        "# Load the data from the file (you can change datasets names here)\n",
        "with open('Formatted_Profession_vs_Gender.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "def get_sentence_probability(model, sentence):\n",
        "    \"\"\"Get probability of a sentence using the GPT-2 model.\"\"\"\n",
        "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    log_likelihood = outputs[0].item()\n",
        "    return log_likelihood\n",
        "\n",
        "evaluation_list = [1,2,3,5]\n",
        "\n",
        "for eval_epoch in evaluation_list:\n",
        "    print(f\"Evaluate epoch {eval_epoch}\")\n",
        "\n",
        "    # This should be 500 for normal training\n",
        "    checkpoint_path = f\"/content/SyntheticDebiasing/results/checkpoint-{eval_epoch*500}\"\n",
        "    eval_model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
        "    eval_model.set_active_adapters(\"debias\")\n",
        "\n",
        "    # Initialize counters\n",
        "    stereotype_count = 0\n",
        "    total_count = len(data)\n",
        "\n",
        "    # Iterate over each entry\n",
        "    for entry in data:\n",
        "        sentence_prob = get_sentence_probability(eval_model, entry[\"sentence\"])\n",
        "        alt_sentence_prob = get_sentence_probability(eval_model, entry[\"alt_sentence\"])\n",
        "\n",
        "        if entry[\"label_1\"] == \"stereotype\":\n",
        "            if sentence_prob < alt_sentence_prob:\n",
        "                stereotype_count += 1\n",
        "        elif entry[\"label_2\"] == \"stereotype\":\n",
        "            if alt_sentence_prob < sentence_prob:\n",
        "                stereotype_count += 1\n",
        "\n",
        "    # Calculate the stereotype score as a percentage\n",
        "    stereotype_score = (stereotype_count / total_count) * 100\n",
        "    print(f\"Stereotype Score for epoch {eval_epoch}: {stereotype_score:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lfLa5PVxmHyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}